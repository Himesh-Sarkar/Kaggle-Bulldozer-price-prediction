{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV,cross_val_score\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score,recall_score,f1_score,plot_roc_curve","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('expand_frame_repr', False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(r\"E:\\Andrei workspace\\Milestone project - 2\\bluebook-for-bulldozers\\bluebook-for-bulldozers\\TrainAndValid.csv\",low_memory=False)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(10,7))\nax.scatter(df[\"saledate\"][:2000],df[\"SalePrice\"][:2000]);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.SalePrice.plot.hist();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate.dtype","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In TS data, better parse dates","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(r\"E:\\Andrei workspace\\Milestone project - 2\\bluebook-for-bulldozers\\bluebook-for-bulldozers\\TrainAndValid.csv\",low_memory=False,parse_dates=['saledate'])\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate.dtype","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(10,7))\nax.scatter(df[\"saledate\"][:2000],df[\"SalePrice\"][:2000]);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.saledate.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sort DataFrame by saledate","metadata":{}},{"cell_type":"code","source":"df.sort_values(by=['saledate'],inplace=True,ascending=True)\ndf.saledate.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp=df.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.MachineID.nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add datetime parameter for saledate column","metadata":{}},{"cell_type":"code","source":"df_tmp['saleYear']=df_tmp.saledate.dt.year\ndf_tmp['saleMonth']=df_tmp.saledate.dt.month\ndf_tmp['saleDay']=df_tmp.saledate.dt.day\ndf_tmp['saleDayOfWeek']=df_tmp.saledate.dt.dayofweek\ndf_tmp['saleDayOfYear']=df_tmp.saledate.dt.dayofyear\ndf_tmp.head().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"sincewe enriched our data, we can remove saledate","metadata":{}},{"cell_type":"code","source":"df_tmp.drop('saledate',axis=1,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check value counts of different columns","metadata":{}},{"cell_type":"code","source":"df_tmp.state.value_counts()   #state ka value counts","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Done enough EDA, now model driven EDA","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_jobs=-1,\n                              random_state=42)\n#model.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp.SalePrice)\ndf_tmp.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.api.types.is_string_dtype(df_tmp[\"UsageBand\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.loc[:,df_tmp.dtypes == object].columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These columns contain strings\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label]= content.astype('category').cat.as_ordered() ## it means, convert into category and then number those category\ndf_tmp.info()  ##check out, we added a new dtype category","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### But despite we cant see in info, the categories are given numeric value","metadata":{}},{"cell_type":"code","source":"df_tmp.state.cat.codes   # liek for datetime we had dt. for categorical types, we have cat.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.state.cat.categories","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now check ratio of missing values","metadata":{}},{"cell_type":"code","source":"(df_tmp.isnull().sum()/len(df_tmp))*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we will drop out columns where missing values are huge. But since we will start from here, lets save pre-processed data","metadata":{}},{"cell_type":"code","source":"df_tmp.to_csv(r\"E:\\Andrei workspace\\Milestone project - 2\\bluebook-for-bulldozers\\bluebook-for-bulldozers\\train_tmp.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp=pd.read_csv(r\"E:\\Andrei workspace\\Milestone project - 2\\bluebook-for-bulldozers\\bluebook-for-bulldozers\\train_tmp.csv\",low_memory=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#numeric values having null values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content) & (pd.isnull(content).sum()>0):\n            print(label)\n            \n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fill numeric row with median and  create a new column to know originality which we replaced","metadata":{}},{"cell_type":"code","source":"for label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content) & (pd.isnull(content).sum()>0):\n            df_tmp[label+\"_is_missing\"]=pd.isnull(content)\n            df_tmp[label]=content.fillna(content.median())   #take a note here, we did content.fillna not df['....'].fillna()\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content) & (pd.isnull(content).sum()>0):\n            print(label)\n            \n## this prints out nothing because we already filled null value with median","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the new columns we created\ndf_tmp.auctioneerID_is_missing.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check columns which *aren't* numeric\nfor label, content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we turned a string column to categories, we had access to an attribute called Codes, which gave us a numerical value for all of the variables in that column. ","metadata":{}},{"cell_type":"code","source":"pd.Categorical(df_tmp['state'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Categorical(df_tmp['state']).dtype","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Categorical(df_tmp['state']).codes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Turn categorical variales into numbers \n- by converting into categorical code, null values will be appointed as -1\n### convert -1 categories (the null values) into 0 by adding +1 to all","metadata":{}},{"cell_type":"code","source":"\nfor label,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        df_tmp[label+\"_is_missing\"]=pd.isnull(content)  #in new columns, we will have true and flase\n        #Turn categories into numbers and add+1, since missing values are assigned -1\n        df_tmp[label]=pd.Categorical(content).codes+1  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.head().T  #lot of x_is_missing columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tmp.isnull().sum()  #now, we have no missing values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finally, no null values and all numeric! Hurray!","metadata":{}},{"cell_type":"code","source":"df_tmp.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=RandomForestRegressor(n_jobs=-1,random_state=42)\nmodel.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp.SalePrice)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.score(df_tmp.drop(\"SalePrice\", axis=1), df_tmp.SalePrice)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though we used whole data, yet score isnt 100%.\n### WHY?","metadata":{}},{"cell_type":"markdown","source":"training data set (as in data-dictionary) has data to end of 2011, validation has data from Jan 1 2012 to april 30 2012, test from May1 to nov2012. <br>\ndf_tmp we used is \"train and val\" set. So we break it into before 2012 and after. ","metadata":{}},{"cell_type":"code","source":"df_tmp.saleYear.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into training and validation\ndf_val = df_tmp[df_tmp.saleYear == 2012]\ndf_train = df_tmp[df_tmp.saleYear != 2012]\n\ndf_val.shape, df_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into X & y\nX_train, y_train = df_train.drop(\"SalePrice\", axis=1), df_train.SalePrice\nX_valid, y_valid = df_val.drop(\"SalePrice\", axis=1), df_val.SalePrice\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"here evaluation metric is RSML(Root sq mean error), but model.score uses coeff of determination\n<br> if you see some metric like RMSE or RMSLE, take it like ratio. like 10% off. But if MAE, its like $10 off","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error,mean_absolute_error,r2_score\ndef rmsle(y_test, y_pred):\n    return np.sqrt(mean_squared_log_error(y_test, y_pred))\n# Create function to evaluate our model\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_valid)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n              \"Valid MAE\": mean_absolute_error(y_valid, val_preds),\n              \"Training RMSLE\": rmsle(y_train, train_preds),\n              \"Valid RMSLE\": rmsle(y_valid, val_preds),\n              \"Training R^2\": r2_score(y_train, train_preds),\n              \"Valid R^2\": r2_score(y_valid, val_preds)}\n    return scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Testing our model on subset to check hyperparameters","metadata":{}},{"cell_type":"code","source":"# # This takes far too long... for experimenting\n\n# %%time\n# model = RandomForestRegressor(n_jobs=-1, \n#                               random_state=42)\n\n# model.fit(X_train, y_train)\nlen(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestRegressor(n_jobs=-1, \n                              random_state=42)\nmodel.get_params()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If bootstrap is true, the number of samples to draw from X to train each base estimator.If none, then draw x shape samples, so it's going to draw them all. <br>\nIf bootstrap is True, the number of samples to draw from X to train each base estimator.\n- If None (default), then draw X.shape[0] samples. ie its gonna draw them all\n- If int, then draw max_samples samples.\n- If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0.0, 1.0].","metadata":{}},{"cell_type":"code","source":"X_train.shape[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"id we leave max_samples as none, then every estimator (ie 100) will see every single one of X_train.shape[0]...but if max_samples=10,000 <br>\nthen, every estimator will see, 401125/10,000 times less. Therefore speed increase by 40 times\n-  Cutting down on the max number of samples each estimator can see improves training time\n","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(n_jobs=-1,random_state=42,max_samples=10000)\nmodel.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#takes 8 minutes, change n_iter=2\n\nfrom sklearn.model_selection import RandomizedSearchCV\nrf_grid = {\"n_estimators\": np.arange(10, 100, 10),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2),\n           \"max_features\": [0.5, 1, \"sqrt\", \"auto\"],\n           \"max_samples\": [10000]}\n\n# Instantiate RandomizedSearchCV model\nrs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n                                                    random_state=42),\n                              param_distributions=rf_grid,\n                              n_iter=10,\n                              cv=5,\n                              verbose=True)\n\n# Fit the RandomizedSearchCV model\nrs_model.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs_model.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(rs_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train a model with the best hyperparamters <br>\n##### Note: These were found after 100 iterations of RandomizedSearchCV\nin the tutors lappy, it took him few hours","metadata":{}},{"cell_type":"code","source":"\n# Most ideal hyperparamters\nideal_model = RandomForestRegressor(n_estimators=40,\n                                    min_samples_leaf=1,\n                                    min_samples_split=14,\n                                    max_features=0.5,\n                                    n_jobs=-1,\n                                    max_samples=None,\n                                    random_state=42) # random state so our results are reproducible\n\n# Fit the ideal model\nideal_model.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(ideal_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make predictions on test data","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(r\"E:\\Andrei workspace\\Milestone project - 2\\bluebook-for-bulldozers\\bluebook-for-bulldozers\\Test.csv\",\n                      low_memory=False,\n                      parse_dates=[\"saledate\"])\n\ndf_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test dataset\n#test_preds = ideal_model.predict(df_test)  #error","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"cz has missing values, doesnt has year, month etc row, has non-numeric columns etc","metadata":{}},{"cell_type":"markdown","source":"Preprocessing the data (getting the test dataset in the same format as our training dataset)","metadata":{}},{"cell_type":"code","source":"def preprocess_data(df):\n    \"\"\"\n    Performs transformations on df and returns transformed df.\n    \"\"\"\n    df['saleYear']=df.saledate.dt.year\n    df['saleMonth']=df.saledate.dt.month\n    df['saleDay']=df.saledate.dt.day\n    df['saleDayOfWeek']=df.saledate.dt.dayofweek\n    df['saleDayOfYear']=df.saledate.dt.dayofyear\n    df.drop('saledate',axis=1,inplace=True)\n    #fill numeric rows with median \n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content) & (pd.isnull(content).sum()>0):\n                df[label+\"_is_missing\"]=pd.isnull(content)\n                df[label]=content.fillna(content.median()) \n    #fill cat data and turn cat data to num\n        if not pd.api.types.is_numeric_dtype(content):\n                df[label+\"_is_missing\"]=pd.isnull(content)\n                df[label]= pd.Categorical(content).codes+1  #content.astype('category').cat.as_ordered() isnt needed. as line 19 fulfills it\n                #nfact we use content.astype('category) only to order via cat.as_ordered()\n\n    return df    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process the test data \ndf_test = preprocess_data(df_test)\ndf_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on updated test data\ntest_preds = ideal_model.predict(df_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can find how the columns differ using sets\nset(X_train.columns) - set(df_test.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Manually adjust df_test to have auctioneerID_is_missing column, its missing cz it didnt had missing_values\ndf_test[\"auctioneerID_is_missing\"] = False\ndf_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test data\ntest_preds = ideal_model.predict(df_test)\ntest_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Format predictions into the same format Kaggle is after\ndf_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalesPrice\"] = test_preds\ndf_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now test dataframe has same features as training df, so ow we can predict","metadata":{}},{"cell_type":"code","source":"test_preds=ideal_model.predict(df_test)\ntest_preds #its in array form, we need in dataframe","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now formatting the way kaggle wants","metadata":{}},{"cell_type":"code","source":"# Format predictions into the same format Kaggle is after\ndf_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalesPrice\"] = test_preds\ndf_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preds.to_csv(r\"E:\\Andrei workspace\\Milestone project - 2\\bluebook-for-bulldozers\\bluebook-for-bulldozers\\we_predicted.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find feature importance of our best model\nideal_model.feature_importances_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(ideal_model.feature_importances_),df_test.shape  #therefore each colums ka correlation bata rha","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we create helper function for top 20 most correlated features\ndef plot_feature(columns,importances,n=20):\n    df=(pd.DataFrame({'features':columns,'feature_importances':importances})).sort_values('feature_importances',ascending=False).reset_index(drop=True)\n    fig,ax=plt.subplots(figsize=(15,10))\n    ax.barh(df['features'][:20],df['feature_importances'][:20])\n    ax.set(ylabel='features',xlabel='Feature importance')\n    ax.invert_yaxis()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature(X_train.columns,ideal_model.feature_importances_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}